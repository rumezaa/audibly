{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"collapsed_sections":["WmDXyMpNOOwO","Vov3uBWn3nOB","mcfPAeLORRB5","nD33HOoYBojm","1dBZunDsCv_G","z-etZqEHpKkP","8hnasfxBrLum"],"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2632847,"sourceType":"datasetVersion","datasetId":1589971},{"sourceId":14524716,"sourceType":"datasetVersion","datasetId":9276689},{"sourceId":14533143,"sourceType":"datasetVersion","datasetId":9282138}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Kaggle + WLASL setup (patched)\nThis notebook is patched to run in **Kaggle** using **/kaggle/input/wlasl-processed** and a local subset JSON.\n\n**You must add your subset JSON** (e.g., `wlasl_subset.json`) to the notebook. Easiest options:\n- Upload it as a Kaggle Dataset and point `SUBSET_JSON` to that path, **or**\n- Upload it during the session and place it at `/kaggle/working/wlasl_subset.json`.\n","metadata":{}},{"cell_type":"code","source":"import os, glob, json\n\n# Kaggle paths\nINPUT_DIR = \"/kaggle/input/wlasl-processed\"\nBASE_DIR  = \"/kaggle/working/sign_language_detection\"\nos.makedirs(BASE_DIR, exist_ok=True)\n\n# Put your filtered subset JSON here.\n# Option A: if you upload it to the notebook session, keep it in /kaggle/working\n# use SUBSET_JSON = \"/kaggle/input/wlasl-subset for a tinier set\"\nSUBSET_JSON = \"/kaggle/input/wlasl-subset\"\n\n# Option B: if you attached it as a Kaggle dataset, set something like:\n# SUBSET_JSON = \"/kaggle/input/<your-dataset-name>/wlasl_subset.json\"\n\nassert os.path.exists(INPUT_DIR), f\"Missing INPUT_DIR: {INPUT_DIR}\"\nassert os.path.exists(SUBSET_JSON), f\"Missing SUBSET_JSON: {SUBSET_JSON} (upload it or attach as dataset)\"\n\nDATA_PATH = os.path.join(BASE_DIR, \"MP_Data\")\nos.makedirs(DATA_PATH, exist_ok=True)\n\nprint(\"INPUT_DIR:\", INPUT_DIR)\nprint(\"SUBSET_JSON:\", SUBSET_JSON)\nprint(\"BASE_DIR:\", BASE_DIR)\nprint(\"DATA_DIR:\", DATA_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Backwards-compat alias used by the original notebook\nbase_dir = BASE_DIR\nbase_dir\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Installing Dependencies","metadata":{"id":"WmDXyMpNOOwO"}},{"cell_type":"code","source":"!pip3 uninstall -y mediapipe\n!pip3 install -U \"numpy<2\" \"protobuf<5\"\n!pip3 install --no-cache-dir \"mediapipe==0.10.21\"","metadata":{"id":"XieFiyuITgUc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This command installs the TensorFlow library using pip (Python's package installer)\n!pip3 install tensorflow","metadata":{"id":"F_Wct-alOAcC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install the OpenCV library, which is used for computer vision tasks such as image and video processing.\n!pip install opencv-python","metadata":{"id":"uF37mdSGOc-t","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Importing Libraries","metadata":{"id":"Vov3uBWn3nOB"}},{"cell_type":"code","source":"# General libraries\nimport os  # For interacting with the operating system (e.g., file handling)\nimport time  # For time-related functions\nimport numpy as np  # For numerical operations and handling arrays\nimport random\nimport pandas as pd\n\n# Visualization libraries\nimport matplotlib.pyplot as plt  # For plotting graphs and images\nfrom IPython.display import display, Image  # For displaying images and other media in Jupyter/Colab\n\n# Computer Vision Libraries\nimport cv2  # For computer vision tasks such as image and video processing\nimport mediapipe as mp  # For using MediaPipe's machine learning models for pose detection and more\n\n\n\n# Deep Learning Libraries (TensorFlow and Keras)\nfrom tensorflow.keras.utils import to_categorical  # For converting labels to categorical format (one-hot encoding)\nfrom tensorflow.keras.models import Sequential  # For defining the neural network architecture\nfrom tensorflow.keras.layers import LSTM, Dense  # For adding LSTM and Dense layers to the model\nfrom tensorflow.keras.callbacks import TensorBoard  # For logging training progress for TensorBoard\nfrom tensorflow.keras.models import load_model  # For loading pre-trained models","metadata":{"id":"2sPZ14hcOTx8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_videos_path = os.path.join(BASE_DIR, 'test_videos')\nos.makedirs(test_videos_path, exist_ok=True)\nprint('test_videos_path:', test_videos_path)\n","metadata":{"id":"eCNZ7kc3bhIm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Detecting Keypoints using MP Holistic","metadata":{"id":"mcfPAeLORRB5"}},{"cell_type":"markdown","source":"Code Block 2A","metadata":{"id":"rmAC6TuEMrO7"}},{"cell_type":"code","source":"# Initialize the Holistic model from MediaPipe for full-body pose detection.\n# This model can detect landmarks for the face, hands, and body.\nmp_holistic = mp.solutions.holistic\n\n# Initialize the drawing utilities from MediaPipe to draw the landmarks on images or videos.\n# These utilities help visualize the pose detection results by drawing the keypoints and connections.\nmp_drawing = mp.solutions.drawing_utils\n\n# This function processes an image using a MediaPipe model to detect landmarks/poses and returns the processed image and the results.\ndef mediapipe_detection(image, model):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n    image.flags.writeable = False                  # Image is no longer writeable\n    results = model.process(image)                 # Make prediction\n    image.flags.writeable = True                   # Image is now writeable\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n    return image, results\n\n# This function draws the detected pose, left hand, and right hand landmarks with their connections on the given image.\ndef draw_landmarks(image, results):\n    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n\n# This function draws the pose, left hand, and right hand landmarks with custom styling (color, thickness, and circle radius)\n# on the given image, using the MediaPipe Holistic model's results.\ndef draw_styled_landmarks(image, results):\n    # Draw pose connections\n    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n                             )\n    # Draw left hand connections\n    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n                             )\n    # Draw right hand connections\n    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n                             )","metadata":{"id":"uuSdZbQQZvVF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Extracting Keypoint Values","metadata":{"id":"nD33HOoYBojm"}},{"cell_type":"markdown","source":"Code Block 3A","metadata":{"id":"_IRRUpFCQ-1X"}},{"cell_type":"code","source":"\n\n# This function extracts keypoints (pose, face, left hand, right hand) from the results of MediaPipe landmarks,\n# flattens the data into 1D arrays, and concatenates them into a single array for further processing.\n\ndef extract_keypoints(results):\n    pose = np.array([[r.x, r.y, r.z, r.visibility] for r in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n    lh   = np.array([[r.x, r.y, r.z] for r in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n    rh   = np.array([[r.x, r.y, r.z] for r in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n    return np.concatenate([pose, lh, rh]).astype(np.float32)\n","metadata":{"id":"BBnZdGWNBqvS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Video Download Utilities\nimport urllib.request\nimport hashlib\n\nDOWNLOAD_CACHE_DIR = \"/kaggle/working/downloaded_videos\"\nos.makedirs(DOWNLOAD_CACHE_DIR, exist_ok=True)\n\ndef is_url(path_or_url: str) -> bool:\n    \"\"\"Check if a string is a URL\"\"\"\n    return path_or_url.startswith(('http://', 'https://'))\n\ndef download_video(url: str, cache_dir: str = DOWNLOAD_CACHE_DIR, timeout: int = 30) -> str:\n    \"\"\"Download a video from URL and cache it locally\"\"\"\n    # Create a hash of the URL for the filename\n    url_hash = hashlib.md5(url.encode()).hexdigest()\n    cached_path = os.path.join(cache_dir, f\"{url_hash}.mp4\")\n    \n    # Return cached file if it exists\n    if os.path.exists(cached_path):\n        return cached_path\n    \n    # Download the video\n    try:\n        print(f\"  ðŸ“¥ Downloading: {url[:60]}...\")\n        urllib.request.urlretrieve(url, cached_path)\n        print(f\"  âœ… Downloaded to: {cached_path}\")\n        return cached_path\n    except Exception as e:\n        print(f\"  âŒ Failed to download {url}: {e}\")\n        return None\n\nprint(\"âœ… Video download utilities ready\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enhanced find_video_path with URL download support\n# This extends the existing find_video_path function\n\ndef find_video_path_enhanced(video_id: str, instance_data: dict = None):\n    \"\"\"\n    Enhanced version that also checks for URLs in instance data.\n    Use this instead of find_video_path if you want URL downloading.\n    \"\"\"\n    # First try the original function\n    result = find_video_path(video_id)\n    if result:\n        return result\n    \n    # Check if video_id itself is a URL\n    if is_url(video_id):\n        downloaded = download_video(video_id)\n        if downloaded:\n            video_path_cache[video_id] = downloaded\n            return downloaded\n    \n    # Check instance data for URL field\n    if instance_data and \"url\" in instance_data:\n        url = instance_data[\"url\"]\n        if is_url(url):\n            downloaded = download_video(url)\n            if downloaded:\n                video_path_cache[video_id] = downloaded\n                return downloaded\n    \n    return None\n\n# Replace the original function (comment out if you want to keep both)\nfind_video_path_original = find_video_path  # Keep backup\nfind_video_path = find_video_path_enhanced  # Use enhanced version\n\nprint(\"âœ… Enhanced find_video_path with URL support ready\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. WLASL subset: build dataset (videos â†’ MediaPipe keypoints â†’ X/y)\nThis replaces the manual folder-collection workflow. It will:\n- read your **filtered WLASL JSON**\n- find matching `.mp4` files under `/kaggle/input/wlasl-processed`\n- extract **30 frames** per clip with MediaPipe Holistic\n- build `X_train, y_train, X_val, y_val, X_test, y_test` using the JSON `split` field\n","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/wlasl-subset/wlasl_subset.json\") as f:\n    data = json.load(f)\n\nactions = sorted({item[\"gloss\"] for item in data})\nactions = np.array(actions)\n\nprint(\"Actions:\", actions)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: Define the set of actions (sign language gestures) to detect\n\nimport os, shutil\n\nDATA_PATH = os.path.join(BASE_DIR, \"MP_Data\")\n\nif os.path.exists(DATA_PATH):\n    shutil.rmtree(DATA_PATH)\n\nos.makedirs(DATA_PATH, exist_ok=True)\n\nprint(\"âœ… Deleted and recreated:\", DATA_PATH)\n\n\n\nactions = np.array(actions)\n\n# Path for exported data (numpy arrays will be saved here)\nDATA_PATH = os.path.join(BASE_DIR, 'MP_Data')\nVIDEO_PATH = os.path.join(BASE_DIR, 'videos')\n\n# Create the data directory if it doesn't exist\nif not os.path.exists(DATA_PATH):\n    os.makedirs(DATA_PATH)\n\n# Number of sequences (videos) to record per action\nno_sequences = 30\n\n# Number of frames per video sequence\nsequence_length = 30\n\n# Starting index for naming folders that store each sequence\nstart_folder = 30\n\n# Create folder structure for each action and each sequence\nfor action in actions:\n    # Path for this action's data\n    action_path = os.path.join(DATA_PATH, action)\n    video_path = os.path.join(VIDEO_PATH, action)\n\n    print(action_path)\n\n    # Create action folder if it doesn't exist\n    if not os.path.exists(action_path):\n        os.makedirs(action_path)\n\n    # Create video folder if it doesn't exist\n    if not os.path.exists(video_path):\n        os.makedirs(video_path)\n\n    # Find existing numbered folders (sequences) for this action\n    existing_dirs = [d for d in os.listdir(action_path) if d.isdigit()]\n\n    # Determine the current max sequence number to avoid overwriting\n    if existing_dirs:\n        dirmax = np.max(np.array(existing_dirs).astype(int))\n    else:\n        dirmax = 0\n\n    # Create subfolders for each new sequence\n    for sequence in range(0, no_sequences):\n        seq_path = os.path.join(action_path, str(dirmax + sequence))\n        if not os.path.exists(seq_path):\n            os.makedirs(seq_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, json, glob\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nWLASL_ROOT = \"/kaggle/input/wlasl-processed\"\nVIDEOS_DIR = os.path.join(WLASL_ROOT, \"videos\")\nDATA_PATH = \"/kaggle/working/sign_language_detection/MP_Data\"\n\nSEQUENCE_LENGTH = 30\nMAX_PER_CLASS = None  # e.g. 50 for testing\n\n# Load your subset JSON (already in `data` per your notebook)\n# data = json.load(open(\"wlasl_subset.json\"))\n\nprint(\"ðŸ” Building video path index...\")\nall_mp4 = glob.glob(os.path.join(WLASL_ROOT, \"**/*.mp4\"), recursive=True)\n# Create a faster lookup: video_id -> path\nvideo_path_cache = {}\nfor p in all_mp4:\n    basename = os.path.basename(p)\n    # Try exact match first\n    vid_id = basename.replace(\".mp4\", \"\")\n    video_path_cache[vid_id] = p\n    # Also index by partial matches\n    if \"_\" in vid_id:\n        parts = vid_id.split(\"_\")\n        for part in parts:\n            if part and part not in video_path_cache:\n                video_path_cache[part] = p\n\ndef find_video_path(video_id: str):\n    # Try exact match first\n    if video_id in video_path_cache:\n        return video_path_cache[video_id]\n    # Try with .mp4 extension\n    direct = os.path.join(VIDEOS_DIR, f\"{video_id}.mp4\")\n    if os.path.exists(direct):\n        video_path_cache[video_id] = direct\n        print(\"video exists!\")\n        return direct\n    # Try partial matches\n    for cached_id, path in video_path_cache.items():\n        if video_id in cached_id or cached_id in video_id:\n            return path\n    return None\n\ndef clamp_bbox(bbox, w, h):\n    x1, y1, x2, y2 = bbox\n    x1 = int(max(0, min(w-1, x1)))\n    x2 = int(max(0, min(w-1, x2)))\n    y1 = int(max(0, min(h-1, y1)))\n    y2 = int(max(0, min(h-1, y2)))\n    if x2 <= x1 or y2 <= y1:\n        return None\n    return x1, y1, x2, y2\n\ndef sample_indices(start, end, n=30):\n    # inclusive start/end frames\n    if end < start:\n        return None\n    total = end - start + 1\n    if total <= 0:\n        return None\n    # Use uniform sampling\n    rel = np.linspace(0, total - 1, n).astype(int)\n    return (start + rel).tolist()\n\n# Build per-gloss list of instances\nby_gloss = {}\nfor entry in data:\n    gloss = entry.get(\"gloss\")\n    insts = entry.get(\"instances\", [])\n    if not gloss or not isinstance(insts, list):\n        continue\n    by_gloss.setdefault(gloss, [])\n    for inst in insts:\n        vid = inst.get(\"video_id\")\n        if vid is None:\n            continue\n        by_gloss[gloss].append(inst)\n        \ntotal_instances = sum(len(v) for v in by_gloss.values())\nprint(\"Total instances to process:\", total_instances)\n\n# optionally cap per class\nif MAX_PER_CLASS is not None:\n    for g in by_gloss:\n        by_gloss[g] = by_gloss[g][:MAX_PER_CLASS]\n\nos.makedirs(DATA_PATH, exist_ok=True)\n\n# Track skip reasons for debugging\nskip_reasons = defaultdict(int)\n\ndef process_video(inst, gloss, seq_id, holistic):\n    \"\"\"Process a single video instance. Returns (success, skip_reason)\"\"\"\n    video_id = str(inst[\"video_id\"])\n    video_path = find_video_path(video_id)\n    \n    if video_path is None:\n        return False, \"video_not_found\"\n    \n    # Check if already processed\n    gloss_dir = os.path.join(DATA_PATH, gloss)\n    out_dir = os.path.join(gloss_dir, str(seq_id))\n    if os.path.exists(out_dir):\n        npy_files = glob.glob(os.path.join(out_dir, \"*.npy\"))\n        if len(npy_files) == SEQUENCE_LENGTH:\n            return True, None  # Already processed successfully\n    \n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        return False, \"cannot_open\"\n    \n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n    \n    if total_frames <= 0:\n        cap.release()\n        return False, \"zero_frames\"\n    \n    # Handle frame_start and frame_end\n    fs = inst.get(\"frame_start\", 0)\n    fe = inst.get(\"frame_end\", -1)\n    \n    # If frame_end is -1 or None, use the full video\n    if fe == -1 or fe is None:\n        fe = total_frames - 1\n    \n    # Ensure valid frame range\n    fs = max(0, min(total_frames - 1, int(fs)))\n    fe = max(0, min(total_frames - 1, int(fe)))\n    \n    # Ensure we have enough frames\n    if fe - fs + 1 < SEQUENCE_LENGTH:\n        # Try to expand the range if possible\n        needed = SEQUENCE_LENGTH - (fe - fs + 1)\n        expand_left = min(needed // 2, fs)\n        expand_right = min(needed - expand_left, total_frames - 1 - fe)\n        fs = max(0, fs - expand_left)\n        fe = min(total_frames - 1, fe + expand_right)\n    \n    if fe < fs or (fe - fs + 1) < SEQUENCE_LENGTH:\n        cap.release()\n        return False, \"insufficient_frames\"\n    \n    idxs = sample_indices(fs, fe, SEQUENCE_LENGTH)\n    if idxs is None or len(idxs) < SEQUENCE_LENGTH:\n        cap.release()\n        return False, \"invalid_indices\"\n    \n    idxs_set = set(idxs)\n    os.makedirs(out_dir, exist_ok=True)\n    \n    bbox = inst.get(\"bbox\", None)\n    saved = 0\n    current = 0\n    \n    # Seek to start frame for faster processing\n    cap.set(cv2.CAP_PROP_POS_FRAMES, fs)\n    \n    while cap.isOpened() and saved < SEQUENCE_LENGTH:\n        ret, frame = cap.read()\n        if not ret or frame is None:\n            break\n        \n        # Check if we've gone past the end frame\n        if current + fs > fe:\n            break\n        \n        if (current + fs) in idxs_set:\n            # Apply bbox if available\n            if bbox is not None and isinstance(bbox, (list, tuple)) and len(bbox) == 4:\n                h, w = frame.shape[:2]\n                bb = clamp_bbox(bbox, w, h)\n                if bb is not None:\n                    x1, y1, x2, y2 = bb\n                    # Ensure bbox is valid\n                    if x2 > x1 and y2 > y1:\n                        frame = frame[y1:y2, x1:x2]\n                    else:\n                        # Invalid bbox, skip this frame\n                        current += 1\n                        continue\n            \n            # Process with MediaPipe\n            try:\n                _, results = mediapipe_detection(frame, holistic)\n                keypoints = extract_keypoints(results) if results is not None else np.zeros((258,), np.float32)\n                np.save(os.path.join(out_dir, f\"{saved}.npy\"), keypoints)\n                saved += 1\n            except Exception as e:\n                # Skip this frame if MediaPipe fails\n                pass\n        \n        current += 1\n    \n    cap.release()\n    \n    if saved != SEQUENCE_LENGTH:\n        # Cleanup incomplete sequence\n        for fpath in glob.glob(os.path.join(out_dir, \"*.npy\")):\n            os.remove(fpath)\n        try:\n            os.rmdir(out_dir)\n        except:\n            pass\n        return False, f\"incomplete_{saved}/{SEQUENCE_LENGTH}\"\n    \n    return True, None\n\n# Process videos\nbuilt = 0\nskipped = 0\n\nwith mp_holistic.Holistic(\n    min_detection_confidence=0.5, \n    min_tracking_confidence=0.5,\n    static_image_mode=False  # Faster for video\n) as holistic:\n    with tqdm(total=total_instances, desc=\"Building MP_Data\", unit=\"inst\") as pbar:\n        for gloss, insts in by_gloss.items():\n            gloss_dir = os.path.join(DATA_PATH, gloss)\n            os.makedirs(gloss_dir, exist_ok=True)\n            \n            # Find next available seq_id\n            existing_seqs = [int(d) for d in os.listdir(gloss_dir) \n                           if os.path.isdir(os.path.join(gloss_dir, d)) and d.isdigit()]\n            seq_id = max(existing_seqs) + 1 if existing_seqs else 0\n            \n            for inst in insts:\n                success, reason = process_video(inst, gloss, seq_id, holistic)\n                \n                if success:\n                    seq_id += 1\n                    built += 1\n                else:\n                    skip_reasons[reason] += 1\n                    skipped += 1\n                \n                pbar.update(1)\n                pbar.set_postfix(gloss=gloss, built=built, skipped=skipped)\n\nprint(\"\\nâœ… Built sequences:\", built)\nprint(\"âš ï¸ Skipped:\", skipped)\nprint(\"\\nðŸ“Š Skip reasons breakdown:\")\nfor reason, count in sorted(skip_reasons.items(), key=lambda x: x[1], reverse=True):\n    print(f\"  {reason}: {count}\")\nprint(f\"\\nSuccess rate: {built/(built+skipped)*100:.1f}%\")\nprint(\"MP_Data at:\", DATA_PATH)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom glob import glob\n\nDATA_PATH = \"/kaggle/working/sign_language_detection/MP_Data\"\n\ndef count_sequences(gloss_dir):\n    # counts subfolders that have at least 1 npy\n    seq_dirs = [d for d in os.listdir(gloss_dir) if os.path.isdir(os.path.join(gloss_dir, d))]\n    good = 0\n    for sd in seq_dirs:\n        npys = glob(os.path.join(gloss_dir, sd, \"*.npy\"))\n        if len(npys) > 0:\n            good += 1\n    return good\n\ncounts = {}\nfor gloss in sorted(os.listdir(DATA_PATH)):\n    gd = os.path.join(DATA_PATH, gloss)\n    if os.path.isdir(gd):\n        counts[gloss] = count_sequences(gd)\n\nprint(\"Counts per class:\")\nfor g,c in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n    print(f\"{g:20s} {c}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MIN_SEQS = 0\n\nkeep_classes = [g for g,c in counts.items() if c >= MIN_SEQS]\ndrop_classes = [g for g,c in counts.items() if c < MIN_SEQS]\n\nprint(\"Keeping:\", keep_classes)\nprint(\"Dropping:\", drop_classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np, os\nfrom glob import glob\n\nDATA_PATH = \"/kaggle/working/sign_language_detection/MP_Data\"\nactions = keep_classes\nactions = sorted(actions) \nX, y = [], []\nfor label, action in enumerate(actions):\n    seq_dirs = sorted(glob(os.path.join(DATA_PATH, action, \"*\")))\n    for sdir in seq_dirs:\n        seq = []\n        ok = True\n        for f in range(30):\n            fp = os.path.join(sdir, f\"{f}.npy\")\n            if not os.path.exists(fp):\n                ok = False\n                break\n            seq.append(np.load(fp))\n        if ok:\n            X.append(np.stack(seq))      # (30,258)\n            y.append(label)\n\nX = np.array(X, dtype=np.float32)         # (N,30,258)\ny = np.array(y, dtype=np.int32)\n\nprint(\"X:\", X.shape, \"y:\", y.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\nimport numpy as np\n\nper_class = defaultdict(list)\nfor i, label in enumerate(y):\n    per_class[int(label)].append(i)\n\nmin_n = min(len(v) for v in per_class.values())  # should be 8\nprint(\"Balancing to\", min_n, \"samples per class\")\n\nkeep_idx = []\nfor label, idxs in per_class.items():\n    keep_idx.extend(idxs[:min_n])\n\nkeep_idx = np.array(keep_idx)\nX = X[keep_idx]\ny = y[keep_idx]\n\nprint(\"Balanced X:\", X.shape)\nprint(\"Balanced y:\", y.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\nnum_classes = len(actions)\n\nnorm = tf.keras.layers.Normalization(axis=-1)\nnorm.adapt(X)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input((30, 258)),\n    # Ensure norm is adapted correctly or replace with a Rescaling(1./1.) layer\n    \n    tf.keras.layers.Bidirectional(\n        tf.keras.layers.GRU(128, return_sequences=True) # Increased units\n    ),\n    tf.keras.layers.BatchNormalization(), # Helps stabilize training\n    tf.keras.layers.Dropout(0.2),\n\n    tf.keras.layers.Bidirectional(\n        tf.keras.layers.GRU(64) # Second layer also bidirectional\n    ),\n    tf.keras.layers.Dense(128, activation=\"relu\"),\n    tf.keras.layers.Dropout(0.3),\n\n    tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-3),\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom collections import defaultdict\n\ndef stratified_split(X, y, val_frac=0.25, seed=42):\n    rng = np.random.default_rng(seed)\n    idx_by_class = defaultdict(list)\n\n    for i, label in enumerate(y):\n        idx_by_class[int(label)].append(i)\n\n    train_idx, val_idx = [], []\n\n    for label, idxs in idx_by_class.items():\n        idxs = np.array(idxs)\n        rng.shuffle(idxs)\n        n_val = int(len(idxs) * val_frac)\n        val_idx.extend(idxs[:n_val])\n        train_idx.extend(idxs[n_val:])\n\n    return (\n        X[train_idx], X[val_idx],\n        y[train_idx], y[val_idx]\n    )\n\nX_train, X_val, y_train, y_val = stratified_split(X, y)\n\n\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(\n        patience=10,\n        restore_best_weights=True\n    ),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        patience=4,\n        factor=0.5,\n        min_lr=1e-5\n    ),\n]\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=80,\n    batch_size=8,\n    callbacks=callbacks,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\nprint(\"val_loss:\", val_loss, \"val_acc:\", val_acc)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random, numpy as np\nfrom glob import glob\n\ndef load_saved_sequence_safe(seq_dir, seq_len=30):\n    frames = []\n    for i in range(seq_len):\n        fp = os.path.join(seq_dir, f\"{i}.npy\")\n        if not os.path.exists(fp):\n            return None   # sequence is incomplete\n        frames.append(np.load(fp).astype(np.float32))\n    return np.stack(frames, axis=0)  # (30,258)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, random, numpy as np\nfrom glob import glob\n\nDATA_PATH = \"/kaggle/working/sign_language_detection/MP_Data\"\nSEQ_LEN = 30\n\ndef pick_random_seq(action):\n    seq_dirs = [\n        d for d in glob(os.path.join(DATA_PATH, action, \"*\"))\n        if os.path.isdir(d)\n    ]\n    random.shuffle(seq_dirs)\n    return seq_dirs\n\nrandom.seed(1)\nn = 20\ncorrect = 0\ntested = 0\n\nfor _ in range(n):\n    true_action = random.choice(actions)\n    seq_dirs = pick_random_seq(true_action)\n\n    x = None\n    sd = None\n    for candidate in seq_dirs:\n        x = load_saved_sequence_safe(candidate, SEQ_LEN)\n        if x is not None:\n            sd = candidate\n            break\n\n    if x is None:\n        print(f\"âš ï¸ no valid sequence for class {true_action}\")\n        continue\n\n    probs = model.predict(x[None, ...], verbose=0)[0]\n    pred = actions[int(np.argmax(probs))]\n    conf = float(np.max(probs))\n\n    tested += 1\n    correct += int(pred == true_action)\n    print(f\"true={true_action:12s} pred={pred:12s} conf={conf:.3f} dir={os.path.basename(sd)}\")\n\nprint(f\"\\nAccuracy on disk samples: {correct}/{tested} = {correct/max(1,tested):.3f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nMODEL_PATH = \"/kaggle/working/wlasl_demo.keras\"\nACTIONS_PATH = \"/kaggle/working/actions.json\"\n\nmodel.save(MODEL_PATH)\n\nwith open(ACTIONS_PATH, \"w\") as f:\n    json.dump(actions, f)\n\nprint(\"Saved model to:\", MODEL_PATH)\nprint(\"Saved actions to:\", ACTIONS_PATH)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Code Block 10A","metadata":{"id":"-9QERLyUNIze"}},{"cell_type":"code","source":"# Use the trained model to make predictions on the test data (X_test)\n# The model outputs the predicted probabilities for each class in the test set\n# y_pred will contain the predicted probabilities or class labels for each sample in X_test\ny_pred = model.predict(X_test)\n\n# Convert the one-hot encoded true labels (y_test) into class labels (indices)\n# np.argmax(y_test, axis=1) finds the index of the maximum value along the second axis (for each sample)\n# This index corresponds to the true class label (e.g., [1, 0, 0] -> class 0, [0, 1, 0] -> class 1)\ny_true = np.argmax(y_test, axis=1).tolist()\n\n# Convert the predicted probabilities (y_pred) into predicted class labels (indices)\n# np.argmax(y_pred, axis=1) finds the index of the highest predicted probability for each sample\n# This index corresponds to the predicted class label\ny_pred = np.argmax(y_pred, axis=1).tolist()","metadata":{"id":"HWcNXlx3rqRe","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Code Block 10B","metadata":{"id":"MOEShsDwOqk8"}},{"cell_type":"code","source":"# Calculate multilabel confusion matrix\nconf_matrix = multilabel_confusion_matrix(y_true, y_pred)\n\nfor idx, matrix in enumerate(conf_matrix):\n    print(f\"\\nConfusion Matrix for class '{actions[idx]}':\")\n    df = pd.DataFrame(matrix,\n                      index=[\"Actual Negative\", \"Actual Positive\"],\n                      columns=[\"Predicted Negative\", \"Predicted Positive\"])\n    print(df)","metadata":{"id":"6gvYFJdrPMWJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Code Block 10C","metadata":{"id":"h-PVE3tjtSUc"}},{"cell_type":"code","source":"# Calculate the accuracy score to evaluate the model's performance\n# The accuracy score measures the ratio of correctly predicted labels to the total number of samples\n# It compares the true labels (y_true) with the predicted labels (y_pred) and returns the percentage of correct predictions\naccuracy_score(y_true, y_pred)","metadata":{"id":"7aJe3RlIrzEw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Note on Kaggle\nKaggle notebooks typically **cannot access your webcam**. Use Section 11 with an **uploaded `.mp4`** (as a Kaggle dataset or placed in `/kaggle/working/test_videos`).\n","metadata":{}},{"cell_type":"markdown","source":"# 11. Testing with Videos","metadata":{"id":"3gOmO7kSr02D"}},{"cell_type":"markdown","source":"Code Block 11A","metadata":{"id":"K5NJekbi1SrS"}}]}